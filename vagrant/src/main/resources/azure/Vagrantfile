#Copyright (c) 2017 TIBCO Software Inc.
#
# This guide is optimized for Vagrant 1.7 and above.
# Although versions 1.6.x should behave very similarly, it is recommended
# to upgrade instead of disabling the requirement below.
Vagrant.require_version ">= 1.7.0"
cached_addresses = {}
tenant_id=ENV['AZURE_TENANT_ID']
client_id=ENV['AZURE_CLIENT_ID']
client_secret=ENV['AZURE_SECRET']
subscription_id=ENV['AZURE_SUBSCRIPTION_ID']

PUPPET_DOMAIN="${PUPPET_DOMAIN}"
cluster_machine_names=Array.new 
# by default rsync is disabled until sudo requiretty is disabled during provisioning
$default_disable_rsync=ENV['VAGRANT_RSYNC'] || true

unless Vagrant.has_plugin?("vagrant-azure")
  raise 'vagrant-${VAGRANT_PROVIDER} is not installed! Enter [vagrant plugin install vagrant-${VAGRANT_PROVIDER}] to install.'
end

Vagrant.configure(2) do |config|

    # Vagrant Defaults
    # Disable the new default behavior introduced in Vagrant 1.7, to
    # ensure that all Vagrant machines will use the same SSH key pair.
    # See https://github.com/mitchellh/vagrant/issues/5005
    config.ssh.insert_key = false
    config.ssh.username = "${VAGRANT_REMOTE_USER}"
    config.ssh.private_key_path = './example_box/vagrant_private.key'
    config.ssh.pty = ('true'==ENV['VAGRANT_SSHPTY_DISABLE'])
    config.vm.boot_timeout = 600
    config.vm.network "private_network", type: "dhcp"
    config.vm.box = "${VAGRANT_PROVIDER}.box"
    config.vm.box_url = "example_box/${VAGRANT_PROVIDER}.box"    
    config.vm.synced_folder ".","/vagrant", type: "rsync",
        rsync__exclude: ["sb","${SB_TARBALL}"],rsync__chown: false, disabled: $default_disable_rsync
    # hostmanager plugin to manage /etc/host file for puppet and ep-maven
    # disable during provisioning due to bug in multi-machine deployment
    config.hostmanager.enabled = false
    config.hostmanager.manage_host = ('true'==ENV['UPDATEHOSTSLOCAL'])
    config.hostmanager.manage_guest = ('true'==ENV['UPDATEHOSTSREMOTE'])
    config.hostmanager.ignore_private_ip = false
    config.hostmanager.include_offline = false
    config.vm.provision :hostmanager
    if ('true'==ENV['UPDATEHOSTSREMOTE'])
        config.hostmanager.ip_resolver = proc do |vm, resolving_vm|
            if vm.communicate.ready?
                if cached_addresses[vm.name].nil?
                    if hostname = (vm.ssh_info && vm.ssh_info[:host])
                        vm.communicate.execute("/sbin/ifconfig eth0 | grep 'inet\s' | tail -n 1 | egrep -o '[0-9\.]+' | head -n 1 2>&1") do |type, contents|
                            cached_addresses[vm.name] = contents.split("\n").first[/(\d+\.\d+\.\d+\.\d+)/, 1]
                        end
                    end
                end
            end
            cached_addresses[vm.name]
        end
    elsif ('true'==ENV['UPDATEHOSTSLOCAL'])
        config.hostmanager.ip_resolver = proc do |vm, resolving_vm|
            if vm.communicate.ready?
                if cached_addresses[vm.name].nil?
                    vm.communicate.execute('curl -H Metadata:true "http://169.254.169.254/metadata/instance/network/interface/0/ipv4/ipAddress/0/publicIpAddress?api-version=2017-04-02&format=text"') do |type, pubip|
                        cached_addresses[vm.name] = pubip.split("\n").first[/(\d+\.\d+\.\d+\.\d+)/, 1]
                    end
                end
            end
            cached_addresses[vm.name]
        end
    end 
    # Azure Defaults
    config.vm.provider :${VAGRANT_PROVIDER} do |${VAGRANT_PROVIDER}|
        # Supply aws credentials using maven properties and pass via ant ENV params
        # Or use ENV vars
        ${VAGRANT_PROVIDER}.tenant_id = "#{tenant_id}"
        ${VAGRANT_PROVIDER}.client_id = "#{client_id}"
        ${VAGRANT_PROVIDER}.client_secret = "#{client_secret}"
        ${VAGRANT_PROVIDER}.subscription_id = "#{subscription_id}"
        ${VAGRANT_PROVIDER}.resource_group_name = '${RESOURCE_GROUP}'
        ${VAGRANT_PROVIDER}.vm_image_urn = "${IMAGE_PUBLISHER}:${IMAGE_ID}:${IMAGE_SKU}:${IMAGE_VERSION}"
        ${VAGRANT_PROVIDER}.location = "${REGIONS}"
        ${VAGRANT_PROVIDER}.vm_size = "${INSTANCE_TYPE}"
        ${VAGRANT_PROVIDER}.virtual_network_name = "${VIRTUAL_NETWORK_NAME}"
        ${VAGRANT_PROVIDER}.subnet_name = "${SUBNET_NAME}"
        ${VAGRANT_PROVIDER}.tcp_endpoints = ${AZURE_PORTS}
    end

    N=${INSTANCE_COUNT}
    (1..N).each do |i|
        config.vm.define "${NODENAME_PREFIX}#{i}" do |instance|
            instance.vm.hostname = "${NODENAME_PREFIX}#{i}"
            cluster_machine_names << "${NODENAME_PREFIX}#{i}"
            instance.vm.provider :${VAGRANT_PROVIDER} do |${VAGRANT_PROVIDER}|
                azure.vm_name = "${NODENAME_PREFIX}#{i}"
                azure.dns_name= "${NODENAME_PREFIX}#{i}".downcase
            end            

            # puppet shell bootstrap provisioning
            if i==1 and "${VAGRANT_PROVISIONER}" == "puppet_server"
                instance.hostmanager.aliases = "${NODENAME_PREFIX}#{i}.#{PUPPET_DOMAIN}"
                instance.vm.provision "shell", path: "puppet_server.sh"
            elsif "${VAGRANT_PROVISIONER}" == "puppet_server"
                instance.hostmanager.aliases = "${NODENAME_PREFIX}#{i}.#{PUPPET_DOMAIN}"
                instance.vm.provision "shell", path: "puppet_agent.sh"
            end

            # puppet agent provisioning
            if "${VAGRANT_PROVISIONER}" == "puppet_server"
                instance.vm.provision "${VAGRANT_PROVISIONER}" do |${VAGRANT_PROVISIONER}|
                    ${VAGRANT_PROVISIONER}.puppet_server = "${NODENAME_PREFIX}1.${PUPPET_DOMAIN}".downcase
                    ${VAGRANT_PROVISIONER}.puppet_node = "${NODENAME_PREFIX}#{i}.${PUPPET_DOMAIN}".downcase
                    ${VAGRANT_PROVISIONER}.options = ${PUPPET_OPS}
                end
            elsif "${VAGRANT_PROVISIONER}" == "puppet"  # puppet apply local - does not support parallelism in loop
                ${VAGRANT_PROVISIONER}.environment = "${PUPPET_ENV}"
                ${VAGRANT_PROVISIONER}.environment_path = "puppet/environments"
                #${VAGRANT_PROVISIONER}.module_path = "puppet/environments/production/modules:/etc/puppetlabs/code/environments/production/modules:/opt/puppetlabs/puppet/modules"
                ${VAGRANT_PROVISIONER}.options = ${PUPPET_OPS}
            elsif "${VAGRANT_PROVISIONER}" == "ansible"
                $shell_script= <<SCRIPT
                # disable sudo requiretty
                sed -i.orig -e 's/^Defaults.*requiretty/# Defaults requiretty/g' /etc/sudoers
                # replace azure bad load balanace with original centos mirror
                sed -i.orig -e 's/olcentgbl.trafficmanager.net/mirror.centos.org/g' /etc/yum.repos.d/CentOS-Base.repo
                # Install the EPEL repo
                rpm -Uvh http://mirror.centos.org/centos/7/extras/x86_64/Packages/epel-release-7-9.noarch.rpm; true
                yum clean metadata
                yum makecache fast
SCRIPT
                config.vm.provision "shell", inline: $shell_script
                      
                # trick to get playbook run only once with parallelism
                if i == N
                    instance.vm.provision "ansible" do |ansible|
#                        ansible.verbose = "v"
                        # fix for ansible plugin hard coded 30s timeout
                        ansible.raw_arguments = ["--timeout=120"]
                        ansible.limit = "all"
                        ansible.groups = { "${ANSIBLE_HOST_GROUP}" => cluster_machine_names }
                        if "${ANSIBLE_TAGS}" == "redeploy"
                            ansible.playbook = "ansible/playbooks/Redeploy_Nodes.yml"
                        elsif "${ANSIBLE_TAGS}" == "update"
                            ansible.playbook = "ansible/playbooks/Update_Cluster_Config.yml"
                        elsif "${ANSIBLE_TAGS}" == "display"
                            ansible.playbook = "ansible/playbooks/Display_All_Nodes.yml"
                        elsif "${ANSIBLE_TAGS}" == "remove"
                            ansible.playbook = "ansible/playbooks/Remove_All_Nodes.yml"
                        elsif ENV['ANSIBLE_TAGS'] == "azuresg"
                            ansible.playbook = "ansible/playbooks/azure_sgupdate.yml"
                            ansible.tags = ENV['ANSIBLE_TAGS']
                            ansible.extra_vars = { CLUSTER_NAME: "${ANSIBLE_HOST_GROUP}", sgsuffix: "-vagrantNSG" }
                        else
                            # perform the entire deployment or custom tags
                            ansible.playbook = "${ANSIBLE_PLAYBOOK}"
                            ansible.tags = "${ANSIBLE_TAGS}"
                            ansible.extra_vars = ${ANSIBLE_EXTRA_VARS}
                        end
                    end
                end
            end
        end
    end
end

